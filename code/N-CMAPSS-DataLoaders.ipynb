{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c32ee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/courseac/miniconda3/envs/transimputer-prognostics/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76be5a",
   "metadata": {},
   "source": [
    "# N-CMAPSS Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd863653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCMAPSSTrainDataset(Dataset):\n",
    "    def __init__(self, ds_no, timesteps=10):\n",
    "        fileloc = self.get_fileloc(ds_no)\n",
    "        X_train, y_train = load_dataframes(fileloc)\n",
    "        \n",
    "        self.features = X_train.columns\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train, columns=self.features)\n",
    "        X_train = self.make_data_seq(X_train, timesteps)\n",
    "        \n",
    "        self.X = torch.Tensor(X_train)\n",
    "        self.y = torch.Tensor(y_train)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def get_fileloc(self, ds_no):\n",
    "        locations = {\n",
    "            1: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS01-005.h5',\n",
    "            2: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS02-006.h5',\n",
    "            3: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS03-012.h5',\n",
    "            4: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS04.h5',\n",
    "            5: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS05.h5',\n",
    "            6: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS06.h5',\n",
    "            7: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS07.h5',\n",
    "            8: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS08a-009.h5',\n",
    "            9: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS08c-008.h5',\n",
    "        }\n",
    "        \n",
    "        return locations[ds_no]\n",
    "    \n",
    "    def load_dataframes(self, fileloc):\n",
    "        with h5py.File(fileloc, 'r') as hdf:\n",
    "            # Development set\n",
    "            W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "            X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "            X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "            T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "            Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "            A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "#             # Test set\n",
    "#             W_test = np.array(hdf.get('W_test'))           # W\n",
    "#             X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "#             X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "#             T_test = np.array(hdf.get('T_test'))           # T\n",
    "#             Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "#             A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "            \n",
    "        X_train = pd.DataFrame(columns=np.concatenate((W_var, X_s_var, X_v_var, T_var, A_var), axis=0),\n",
    "                              data=np.concatenate((W_dev, X_s_dev, X_v_dev, T_dev, A_dev), axis=1))\n",
    "        \n",
    "        y_train = pd.DataFrame(columns=['RUL'], data=Y_dev)\n",
    "        \n",
    "        return X_train, y_train\n",
    "    \n",
    "    def make_data_seq(self, df, n_timesteps):\n",
    "        seq_data = []\n",
    "        for unit in df['units'].unique():\n",
    "            i = 0\n",
    "            unit_data = df[df['units'] == unit]\n",
    "\n",
    "            for index, row in unit_data.iterrows():\n",
    "                seq_point = []\n",
    "                pad_num = n_timesteps-i-1\n",
    "\n",
    "                non_padded = np.expand_dims(np.array(unit_data[i-(n_timesteps-pad_num-1):i+1]), axis=0)\n",
    "\n",
    "                if pad_num > 0:\n",
    "                    padded = np.zeros((1, pad_num, row.shape[0]))\n",
    "                    seq_point = np.concatenate([padded, non_padded], axis=1)\n",
    "                else:\n",
    "                    seq_point = non_padded\n",
    "\n",
    "                seq_data.append(seq_point)\n",
    "                i += 1\n",
    "\n",
    "\n",
    "        seq_data = np.array(seq_data).squeeze(1)\n",
    "        \n",
    "        return seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ae1ad972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version only reads the relevant line from the file\n",
    "\n",
    "class NCMAPSSTrainDataset(Dataset):\n",
    "    def __init__(self, ds_no, timesteps=10):\n",
    "        self.fileloc = self.get_fileloc(ds_no)\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start = index - self.timesteps + 1\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        indices = list(range(start, index+1))\n",
    "        \n",
    "        with h5py.File(self.fileloc, 'r') as hdf:\n",
    "            # Development set\n",
    "            W_dev = np.array(hdf.get('W_dev')[indices])\n",
    "            X_s_dev = np.array(hdf.get('X_s_dev')[indices])\n",
    "            X_v_dev = np.array(hdf.get('X_v_dev')[indices])\n",
    "            T_dev = np.array(hdf.get('T_dev')[indices])\n",
    "            Y_dev = np.array(hdf.get('Y_dev')[index])\n",
    "            A_dev = np.array(hdf.get('A_dev')[indices])\n",
    "            \n",
    "            unit = A_dev[-1:, 0]\n",
    "            \n",
    "        X_train = np.concatenate((W_dev, X_s_dev, X_v_dev, T_dev, A_dev), axis=1)\n",
    "        n_pad = self.timesteps - X_train.shape[0]\n",
    "        X_train = np.pad(X_train, ((n_pad, 0),(0,0)), mode='constant')\n",
    "        \n",
    "        for i, row in enumerate(X_train):\n",
    "            curr_unit = X_train[i,42]\n",
    "            if curr_unit != unit:\n",
    "                X_train[i] = np.zeros_like(row)\n",
    "        \n",
    "        return X_train, Y_dev\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000 # implement this dynamically\n",
    "    \n",
    "    def get_fileloc(self, ds_no):\n",
    "        locations = {\n",
    "            1: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS01-005.h5',\n",
    "            2: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS02-006.h5',\n",
    "            3: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS03-012.h5',\n",
    "            4: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS04.h5',\n",
    "            5: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS05.h5',\n",
    "            6: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS06.h5',\n",
    "            7: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS07.h5',\n",
    "            8: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS08a-009.h5',\n",
    "            9: '/data/courseac/N-CMAPSS/data_set/N-CMAPSS_DS08c-008.h5',\n",
    "        }\n",
    "        \n",
    "        return locations[ds_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "36e25eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = NCMAPSSTrainDataset(1, timesteps=10)\n",
    "trainloader = DataLoader(traindata, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e506c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(enumerate(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cf1fae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_X = example[1][0]\n",
    "ex_y = example[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e0a4b1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 46])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "740edafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a8c2e379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99],\n",
       "        [99]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
