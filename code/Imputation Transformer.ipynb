{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a76e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\austi\\miniconda3\\envs\\deep_learn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import DataUtils\n",
    "import Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827b30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af0594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]\n"
     ]
    }
   ],
   "source": [
    "columns_excluded = [0, 1, 26]\n",
    "columns_kept = [False, False, True, True, True,\n",
    "               True, True, True, True, True,\n",
    "               True, True, True, True, True,\n",
    "               True, True, True, True, True,\n",
    "               True, True, True, True, True,\n",
    "               True, False]\n",
    "print(columns_kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2db1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = DataUtils.get_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d26ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(enumerate(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39eb91b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 542, 27])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "ex_X = example[1][0]\n",
    "print(ex_X.shape)\n",
    "\n",
    "ex_y = example[1][1]\n",
    "print(ex_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68d2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we use a linear attention transformer?\n",
    "class ImputationTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(542*24, embed_dim)\n",
    "        self.positional_embed = nn.Parameter(torch.randn(embed_dim))\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, activation=\"gelu\")\n",
    "        self.transformer_blocks = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        z = self.input_projection(x)\n",
    "        z = z + self.positional_embed\n",
    "        z = self.transformer_blocks(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf599553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionImputationTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = 128\n",
    "        self.imputation_transformer = ImputationTransformer(self.embed_dim)\n",
    "        self.reconstruction = nn.Sequential(nn.Linear(self.embed_dim, 2048),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(2048, 542*24))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.imputation_transformer(x)\n",
    "        z = self.reconstruction(z)\n",
    "        \n",
    "        return z.view(x.shape[0],542,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3badfc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 542, 24])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_X[:,:,columns_kept].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91118af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 30954960\n"
     ]
    }
   ],
   "source": [
    "r_tran = ReconstructionImputationTransformer()\n",
    "print(\"Num params:\",sum(p.numel() for p in r_tran.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50423578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 542, 24])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_tran(ex_X[:,:,columns_kept].float()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57544dd",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9624d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_missing(xhat, masked_X, X):\n",
    "    missing_idx = (masked_X==float(-1))\n",
    "    error = xhat[missing_idx] - X[missing_idx]\n",
    "    return torch.nanmean(torch.pow(error, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "744c1426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:43<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.23848849606029304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:16<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.009394740167889418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:29<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 0.00914702090447563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:28<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 0.008450185747194925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:32<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 0.00818822823173568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:34<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 0.008966166759786372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:45<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 0.008078510104771635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:35<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 0.007845037007920162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [20:38<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 0.008686192886435542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████                                                           | 88/323 [05:35<14:56,  3.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m masked_X \u001b[38;5;241m=\u001b[39m Masking\u001b[38;5;241m.\u001b[39mmask_input(X)\n\u001b[0;32m     14\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m xhat \u001b[38;5;241m=\u001b[39m \u001b[43mr_tran\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns_kept\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m objective(xhat, masked_X[:,:,columns_kept], X[:,:,columns_kept])\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [17], line 13\u001b[0m, in \u001b[0;36mReconstructionImputationTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     12\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_transformer(x)\n\u001b[1;32m---> 13\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m542\u001b[39m,\u001b[38;5;241m24\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "objective = mse_missing\n",
    "\n",
    "r_tran = ReconstructionImputationTransformer()\n",
    "\n",
    "lr = 1e-4\n",
    "n_epochs = 25\n",
    "optim = torch.optim.Adam(r_tran.parameters(), lr=lr)\n",
    "losses = []\n",
    "\n",
    "for n in range(n_epochs):\n",
    "    counter = 0\n",
    "    for i, (X, y) in enumerate(tqdm(trainloader)):\n",
    "        masked_X = Masking.mask_input(X)\n",
    "        optim.zero_grad()\n",
    "        xhat = r_tran(masked_X[:,:,columns_kept].float())\n",
    "        loss = objective(xhat, masked_X[:,:,columns_kept], X[:,:,columns_kept])\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optim.step()\n",
    "        counter += 1\n",
    "        \n",
    "    print(\"Epoch:\", n+1, \"Loss:\",np.mean(losses[-counter:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfd19ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(r_tran.state_dict(), './saved_models/recons_imputation_transformer_prototype.pt')\n",
    "torch.save(r_tran.imputation_transformer.state_dict(), './saved_models/imputation_transformer_prototype.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39d4dfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 205/205 [06:21<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.015520776898235857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_mses = []\n",
    "with torch.no_grad():\n",
    "    for i, (X, y) in enumerate(tqdm(testloader)):\n",
    "            masked_X = Masking.mask_input(X)\n",
    "            xhat = r_tran(masked_X[:,:,columns_kept].float())\n",
    "            test_mse = mse_missing(xhat, masked_X[:,:,columns_kept], X[:,:,columns_kept])\n",
    "            test_mses.append(test_mse.item())\n",
    "print(\"Test MSE:\", np.mean(test_mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc54e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImputationTransformer(embed_dim=128)\n",
    "model.load_state_dict(torch.load('./saved_models/imputation_transformer_prototype.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a38f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(ex_X[:,:,columns_kept].float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cd14a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d4e552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(tensor):\n",
    "    interpolated = []\n",
    "    for i in range(tensor.shape[0]):\n",
    "        df = pd.DataFrame(tensor[i])\n",
    "        df[df == -1] = float('nan')\n",
    "        df = df.interpolate(method='linear', axis=0).fillna(method='bfill')\n",
    "        interpolated.append(df.to_numpy())\n",
    "        \n",
    "    return torch.FloatTensor(np.array(interpolated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b781b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.13215842927828966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_mses = []\n",
    "for i, (X, y) in enumerate(tqdm(testloader)):\n",
    "        masked_X = Masking.mask_input(X)\n",
    "        xhat = interpolate(masked_X[:,:,columns_kept])\n",
    "        test_mse = mse_missing(xhat, masked_X[:,:,columns_kept], X[:,:,columns_kept])\n",
    "        test_mses.append(test_mse.item())\n",
    "print(\"Test MSE:\", np.mean(test_mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9fbd00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backfill_impute(tensor):\n",
    "    interpolated = []\n",
    "    for i in range(tensor.shape[0]):\n",
    "        df = pd.DataFrame(tensor[i])\n",
    "        df[df == -1] = float('nan')\n",
    "        df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "        interpolated.append(df.to_numpy())\n",
    "        \n",
    "    return torch.FloatTensor(np.array(interpolated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f824e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.1667811141694234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_mses = []\n",
    "for i, (X, y) in enumerate(tqdm(testloader)):\n",
    "        masked_X = Masking.mask_input(X)\n",
    "        xhat = backfill_impute(masked_X[:,:,columns_kept])\n",
    "        test_mse = mse_missing(xhat, masked_X[:,:,columns_kept], X[:,:,columns_kept])\n",
    "        test_mses.append(test_mse.item())\n",
    "print(\"Test MSE:\", np.mean(test_mses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
